{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nGram Error Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Error Calculation per Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ''\n",
    "file_names = ['0.1.dat', '1.dat', '5.dat', '10.dat', '20.dat', '50.dat']\n",
    "df_names = ['eps_01', 'eps_1', 'eps_5', 'eps_10', 'eps_20', 'eps_50']\n",
    "for name, file in zip(df_names, file_names):\n",
    "    exec(f\"{name} = pd.read_csv('{path+file}', dtype={{'Seq': str}}, sep = ':')\")\n",
    "datasets = {\"eps_01\": eps_01, \"eps_1\": eps_1, \"eps_5\": eps_5, \"eps_10\": eps_10, \"eps_20\": eps_20, \"eps_50\": eps_50}\n",
    "\n",
    "for df in datasets.values():\n",
    "    df['Seq'] = df['Seq'].astype(str)\n",
    "\n",
    "og = pd.read_csv('/home/myercel1/research/KTH-traces/Ziming/notebooks/data/ngram_data/small_example-original-5grams.dat', sep = ':', dtype={'Seq': str})\n",
    "\n",
    "og['ngram'] = og['Seq'].apply(lambda x: len(x.split()))\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    dataset['ngram'] = dataset['Seq'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. With Noisy counts less than 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1. Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_diff_line_plot(n):\n",
    "    # Collect all unique sequences for the given n-gram across all datasets\n",
    "    all_sequences = set()\n",
    "    for dataset in datasets.values():\n",
    "        ngram_dataset = dataset[dataset['ngram'] == n]\n",
    "        all_sequences.update(ngram_dataset['Seq'].unique())\n",
    "\n",
    "    # Create a dictionary to map each unique sequence to a numerical value; sort the keys number by number\n",
    "    sequence_mapping = {sequence: i for i, sequence in enumerate(sorted(all_sequences, key=lambda x: [int(num) for num in x.split()]))}\n",
    "\n",
    "    # Identify the n-grams in the original dataset\n",
    "    og_ngram = og[og['ngram'] == n]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Iterate over each noisy dataset\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        # Identify the n-grams\n",
    "        dataset_ngram = dataset[dataset['ngram'] == n]\n",
    "\n",
    "        # Calculate the differences\n",
    "        # Merge on 'Seq' and use suffixes to differentiate columns\n",
    "        difference = pd.merge(dataset_ngram, og_ngram, on='Seq', how='outer', suffixes=('_' + dataset_name, '_og'))\n",
    "\n",
    "        # Fill NaNs with 0\n",
    "        difference.fillna(0, inplace=True)\n",
    "\n",
    "        # Calculate the absolute difference\n",
    "        difference['abs_difference'] = np.abs(difference['Counts_og'] - difference['Counts_' + dataset_name])\n",
    "\n",
    "        # Sort the sequences based on the order of sequences on the x-axis\n",
    "        difference['seq_numeric'] = difference['Seq'].map(sequence_mapping)\n",
    "        difference.sort_values('seq_numeric', inplace=True)\n",
    "\n",
    "        # Plotting scatter plot\n",
    "        plt.scatter(difference['seq_numeric'], difference['abs_difference'], label=dataset_name)\n",
    "\n",
    "        # Connect the dots with a line\n",
    "        plt.plot(difference['seq_numeric'], difference['abs_difference'], label=None)\n",
    "\n",
    "    # Set the tick labels to the original sequences\n",
    "    plt.xticks(list(sequence_mapping.values()), list(sequence_mapping.keys()), rotation='vertical')\n",
    "\n",
    "    plt.xlabel(f\"{n}-gram sequences\")\n",
    "    plt.ylabel('Absolute difference in counts')\n",
    "    plt.title(f\"Absolute difference in counts for {n}-grams across datasets\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    abs_diff_line_plot(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Results description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the absolute difference decreases as n-gram length gets larger. Likewise, the absolute difference decreases as epsilon value increases (more privacy loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2. Percentage Error with Sanity Bound 0.008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_diff_line_plot(n,sanity_bound):\n",
    "    # Collect all unique sequences for the given n-gram across all datasets\n",
    "    all_sequences = set()\n",
    "    for dataset in datasets.values():\n",
    "        ngram_dataset = dataset[dataset['ngram'] == n]\n",
    "        all_sequences.update(ngram_dataset['Seq'].unique())\n",
    "\n",
    "    # Create a dictionary to map each unique sequence to a numerical value; sort the keys number by number\n",
    "    sequence_mapping = {sequence: i for i, sequence in enumerate(sorted(all_sequences, key=lambda x: [int(num) for num in x.split()]))}\n",
    "\n",
    "    # Identify the n-grams in the original dataset\n",
    "    og_ngram = og[og['ngram'] == n]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Iterate over each noisy dataset\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        # Identify the n-grams\n",
    "        dataset_ngram = dataset[dataset['ngram'] == n]\n",
    "\n",
    "        # Calculate the differences\n",
    "        difference = pd.merge(dataset_ngram, og_ngram, on='Seq', how='outer', suffixes=('_' + dataset_name, '_og'))\n",
    "\n",
    "        # Fill NaNs with 0\n",
    "        difference.fillna(0, inplace=True)\n",
    "        \n",
    "        # Calculate the absolute difference\n",
    "        difference['abs_difference'] = np.abs(difference['Counts_og'] - difference['Counts_' + dataset_name])\n",
    "\n",
    "        # Calculate the percentage\n",
    "        difference['percentage_difference'] = difference.apply(lambda row: 100 * row['abs_difference'] / max(row['Counts_og'], sanity_bound), axis=1)\n",
    "\n",
    "        # Sort the sequences based on the order of sequences on the x-axis\n",
    "        difference['seq_numeric'] = difference['Seq'].map(sequence_mapping)\n",
    "        difference.sort_values('seq_numeric', inplace=True)\n",
    "\n",
    "        # Plotting scatter plot\n",
    "        plt.scatter(difference['seq_numeric'], difference['percentage_difference'], label=dataset_name)\n",
    "\n",
    "        # Connect the dots with a line\n",
    "        plt.plot(difference['seq_numeric'], difference['percentage_difference'], label=None)\n",
    "\n",
    "    # Set the tick labels to the original sequences\n",
    "    plt.xticks(list(sequence_mapping.values()), list(sequence_mapping.keys()), rotation='vertical')\n",
    "\n",
    "    plt.xlabel(f\"{n}-gram sequences\")\n",
    "    plt.ylabel('Percentage difference in counts')\n",
    "    plt.title(f\"Percentage difference in counts for {n}-grams across datasets\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    perc_diff_line_plot(i, 0.008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Results description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to absolute differences, percent differences increase as n-gram length increases. As expected, percent difference decreases with increasing epsilon value (more privacy loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3. Percentage Error with Sanity Bound 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    perc_diff_line_plot(i, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Results description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the sanity bound to 3 significantly lowered percent error. Now, similar to absolute error, percent error decreases with increasing n-gram length and decreases with increasing epsilon value (more privacy loss).\n",
    "\n",
    "Sanity bound of 3 is the average count per ngram in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Without Noisy counts less than 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_diff_line_plot_wo(n):\n",
    "    # Collect all unique sequences for the given n-gram across all datasets\n",
    "    all_sequences = set()\n",
    "    for dataset in datasets.values():\n",
    "        ngram_dataset = dataset[dataset['ngram'] == n]\n",
    "        all_sequences.update(ngram_dataset['Seq'].unique())\n",
    "\n",
    "    # Create a dictionary to map each unique sequence to a numerical value; sort the keys number by number\n",
    "    sequence_mapping = {sequence: i for i, sequence in enumerate(sorted(all_sequences, key=lambda x: [int(num) for num in x.split()]))}\n",
    "\n",
    "    # Identify the n-grams in the original dataset\n",
    "    og_ngram = og[og['ngram'] == n]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Iterate over each noisy dataset\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        # Identify the n-grams\n",
    "        dataset_ngram = dataset[dataset['ngram'] == n]\n",
    "\n",
    "        # Calculate the differences\n",
    "        # Merge on 'Seq' and use suffixes to differentiate columns\n",
    "        difference = pd.merge(dataset_ngram, og_ngram, on='Seq', how='outer', suffixes=('_' + dataset_name, '_og'))\n",
    "\n",
    "        # Fill NaNs with 0\n",
    "        difference.fillna(0, inplace=True)\n",
    "\n",
    "        # Calculate the absolute difference\n",
    "        difference['abs_difference'] = np.abs(difference['Counts_og'] - difference['Counts_' + dataset_name])\n",
    "        \n",
    "        # Filter out data entries where noisy count is less than 1\n",
    "        difference = difference[difference['Counts_' + dataset_name] >= 1]\n",
    "\n",
    "        # Sort the sequences based on the order of sequences on the x-axis\n",
    "        difference['seq_numeric'] = difference['Seq'].map(sequence_mapping)\n",
    "        difference.sort_values('seq_numeric', inplace=True)\n",
    "\n",
    "        # Plotting scatter plot\n",
    "        plt.scatter(difference['seq_numeric'], difference['abs_difference'], label=dataset_name)\n",
    "\n",
    "        # Connect the dots with a line\n",
    "        plt.plot(difference['seq_numeric'], difference['abs_difference'], label=None)\n",
    "\n",
    "    # Set the tick labels to the original sequences\n",
    "    plt.xticks(list(sequence_mapping.values()), list(sequence_mapping.keys()), rotation='vertical')\n",
    "\n",
    "    plt.xlabel(f\"{n}-gram sequences\")\n",
    "    plt.ylabel('Absolute difference in counts')\n",
    "    plt.title(f\"Absolute difference in counts for {n}-grams across datasets\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    abs_diff_line_plot_wo(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Results description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the result from 1.1.1, we can see that the absolute difference in counts decreases as n-gram length gets larger. It is also apparent that the absolute difference decreases as epsilon value increases, indicating more privacy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. Percentage Error with Sanity Bound 0.008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_diff_line_plot_wo(n,sanity_bound):\n",
    "    # Collect all unique sequences for the given n-gram across all datasets\n",
    "    all_sequences = set()\n",
    "    for dataset in datasets.values():\n",
    "        ngram_dataset = dataset[dataset['ngram'] == n]\n",
    "        all_sequences.update(ngram_dataset['Seq'].unique())\n",
    "\n",
    "    # Create a dictionary to map each unique sequence to a numerical value; sort the keys number by number\n",
    "    sequence_mapping = {sequence: i for i, sequence in enumerate(sorted(all_sequences, key=lambda x: [int(num) for num in x.split()]))}\n",
    "\n",
    "    # Identify the n-grams in the original dataset\n",
    "    og_ngram = og[og['ngram'] == n]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Iterate over each noisy dataset\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        # Identify the n-grams\n",
    "        dataset_ngram = dataset[dataset['ngram'] == n]\n",
    "\n",
    "        # Calculate the differences\n",
    "        difference = pd.merge(dataset_ngram, og_ngram, on='Seq', how='outer', suffixes=('_' + dataset_name, '_og'))\n",
    "\n",
    "        # Fill NaNs with 0\n",
    "        difference.fillna(0, inplace=True)\n",
    "        \n",
    "        # Calculate the absolute difference\n",
    "        difference['abs_difference'] = np.abs(difference['Counts_og'] - difference['Counts_' + dataset_name])\n",
    "\n",
    "        # Calculate the percentage\n",
    "        difference['percentage_difference'] = difference.apply(lambda row: 100 * row['abs_difference'] / max(row['Counts_og'], sanity_bound), axis=1)\n",
    "        \n",
    "        # Filter out data entries where noisy count is less than 1\n",
    "        difference = difference[difference['Counts_' + dataset_name] >= 1]\n",
    "\n",
    "        # Sort the sequences based on the order of sequences on the x-axis\n",
    "        difference['seq_numeric'] = difference['Seq'].map(sequence_mapping)\n",
    "        difference.sort_values('seq_numeric', inplace=True)\n",
    "\n",
    "        # Plotting scatter plot\n",
    "        plt.scatter(difference['seq_numeric'], difference['percentage_difference'], label=dataset_name)\n",
    "\n",
    "        # Connect the dots with a line\n",
    "        plt.plot(difference['seq_numeric'], difference['percentage_difference'], label=None)\n",
    "\n",
    "    # Set the tick labels to the original sequences\n",
    "    plt.xticks(list(sequence_mapping.values()), list(sequence_mapping.keys()), rotation='vertical')\n",
    "\n",
    "    plt.xlabel(f\"{n}-gram sequences\")\n",
    "    plt.ylabel('Percentage difference in counts')\n",
    "    plt.title(f\"Percentage difference in counts for {n}-grams across datasets\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    perc_diff_line_plot_wo(i,0.008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Results description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percent differences increase as n-gram length increases similar to 1.1.2. Percent difference decreases with increasing epsilon value. Compared to 1.1.2, there are much less plots in these results as the noisy counts less than 1 are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3. Percentage Error with Sanity Bound 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    perc_diff_line_plot_wo(i,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3. Results description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the sanity bound to 3 significantly lowered percent error, similar to 1.1.3. Percent error decreases with increasing n-gram length and decreases with increasing epsilon value. For 4-grams, the percentage error was lowered but maintai there are no changes compared with 1.2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Error Calculation per NGram type (n-size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze how the length of the gram (gram of size n) impacts the error between noisy and original n-gram counts, we calculated the error per n-gram type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Taking the Average of Instance Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method we used to calculate percentage error per gram type was taking the average of the count errors for each instance (one specific n-gram)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Sanity Bound 0.008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For types of grams having count lower than the sanity bound, we replace their counts with sanity bound, which is, by paper standards, 0.1% of the dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adj_avg_percentage_error(epsilons, ngrams, sanity_bound):\n",
    "    # Initialize a figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Iterate over each epsilon\n",
    "    for epsilon in epsilons:\n",
    "        avg_errors = []\n",
    "        \n",
    "        # Iterate over each n-gram\n",
    "        for n in ngrams:\n",
    "            # Collect all percentage differences for the given epsilon and n-gram\n",
    "            all_percentage_differences = []\n",
    "            \n",
    "            # Identify the n-grams in the original dataset\n",
    "            og_ngram = og[og['ngram'] == n]\n",
    "            \n",
    "            # Identify the n-grams for particular epsilon\n",
    "            dataset_ngram = datasets[epsilon][datasets[epsilon]['ngram'] == n]\n",
    "            \n",
    "            # Calculate the differences\n",
    "            # Merge on 'Seq' and use suffixes to differentiate columns\n",
    "            difference = pd.merge(dataset_ngram, og_ngram, on='Seq', how='outer', suffixes=('_' + epsilon, '_og'))\n",
    "\n",
    "            # Fill NaNs with 0\n",
    "            difference.fillna(0, inplace=True)\n",
    "\n",
    "            # Calculate the absolute difference\n",
    "            difference['abs_difference'] = np.abs(difference['Counts_og'] - difference['Counts_' + epsilon])\n",
    "\n",
    "            # Calculate the percentage difference\n",
    "            difference['percentage_difference'] = difference.apply(lambda row: 100 * row['abs_difference'] / max(row['Counts_og'], sanity_bound), axis=1)\n",
    "\n",
    "            # Filter out noisy counts that are less than 1\n",
    "            difference = difference[difference['Counts_' + epsilon] >= 1]\n",
    "            \n",
    "            # Append the percentage differences to the list\n",
    "            all_percentage_differences.extend(difference['percentage_difference'])\n",
    "\n",
    "            # Calculate the average percentage error for the current epsilon and n-gram\n",
    "            # avg_error = np.mean(all_percentage_differences)\n",
    "            avg_error = np.mean(all_percentage_differences) if all_percentage_differences else np.nan\n",
    "\n",
    "            # Append the average error to the list\n",
    "            avg_errors.append(avg_error)\n",
    "\n",
    "        # Plotting\n",
    "        plt.plot(ngrams, avg_errors, marker='o', label=epsilon)\n",
    "\n",
    "    plt.xlabel(\"n-gram\")\n",
    "    plt.ylabel(\"Average Percentage Error\")\n",
    "    plt.title(\"Average Percentage Error for n-grams across Epsilons\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [\"eps_01\", \"eps_1\", \"eps_5\", \"eps_10\", \"eps_20\", \"eps_50\"]\n",
    "ngrams = range(1, 5)\n",
    "plot_adj_avg_percentage_error(epsilons, ngrams, 0.008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Results description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that with the size of the gram increasing, the percentage error increases, so the general trend is satisfactory to the common intuition we had about the results. However, the values of the errors themselves are outrageously large, which prompted us to try another sanity bound value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Sanity Bound 3 for All Epsilons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our specific dataset, we figured that 0.1% of the original dataset's size might not be an appropriate sanity bound, as our dataset is much smaller than the one used in the original paper. Therefore, we took average n-gram count in the whole dataset, which was 3, and tried it as a sanity bound as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [\"eps_01\", \"eps_1\", \"eps_5\", \"eps_10\", \"eps_20\", \"eps_50\"]\n",
    "plot_adj_avg_percentage_error(epsilons, ngrams, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Results description:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that setting a sanity bound to a value more specific to our dataset was a good decision, as the percentage error values are within the range of what we would consider appropriate by common sense and still follow the general trend of the error increasing with the size of the n-gram. Four-grams have errors around 60% and 90% which is quite a gap, while unigram errors are in an approximately 10% to 15% error with epsilon 5 being the outlier and having 60% error for uni-, bi-, and gour-grams. In other words, generally, the higher the n, the higher the error percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Taking the Sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method we used to analyze the percentage errors by n-gram type is taking the sums of the counts of all grams of the same n and finding the error between those sums and their noisy versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = [0.1, 1, 5, 10, 20]\n",
    "path = \"\"\n",
    "orig_name = path + \"small_example-original-5grams.dat\"\n",
    "filenames = []\n",
    "for epsilon in eps:\n",
    "    noisy_name_epsilon = path + \"small_example-noisy-n_max_5-l_max_5-eps_\" + str(epsilon) + \".dat\"\n",
    "    filenames.append(noisy_name_epsilon)\n",
    "    \n",
    "def make_dict(filename, gram_type):\n",
    "    file = open(filename, 'r')\n",
    "    l = file.readlines()[2:]\n",
    "    dct = {}\n",
    "    for line in l:\n",
    "        lst = line.strip().split(\":\")\n",
    "        gram = lst[0]\n",
    "        if len(gram.replace(\" \",\"\")) == gram_type:\n",
    "            count = float(lst[1])\n",
    "            dct[gram] = count\n",
    "    return dct\n",
    "        \n",
    "def per_gram_type(orig_name, filenames, n):\n",
    "    orig = make_dict(orig_name, n)\n",
    "    df1 = pd.DataFrame.from_dict(orig, orient='index', columns=['count']).reset_index().rename(columns={'index':'gram'})\n",
    "    noisy_frames = []\n",
    "    for noisy_name in filenames:\n",
    "        noisy = make_dict(noisy_name, n)\n",
    "        df2 = pd.DataFrame.from_dict(noisy, orient='index',columns=['count']).reset_index().rename(columns={'index':'gram'})\n",
    "        noisy_frames.append(df2)\n",
    "    return df1, noisy_frames\n",
    "    \n",
    "def sum_up(orig_name,filenames):\n",
    "    final = pd.DataFrame()\n",
    "    types_list = ['uni', 'bi','tri','quad','five']\n",
    "    final['gramtype'] = ['uni', 'bi','tri','quad','five']\n",
    "    for i in range(5):\n",
    "        df1, noisy_frames = per_gram_type(orig_name,filenames, i+1)\n",
    "        eps_diffs = []\n",
    "        counter = 0\n",
    "        for df2 in noisy_frames:\n",
    "            df1.fillna(0, inplace = True)\n",
    "            df2.fillna(0, inplace = True)\n",
    "            orig = df1['count'].sum()\n",
    "            noisy = df2['count'].sum()\n",
    "            diff = (np.abs(orig - noisy)/orig)*100\n",
    "            epsilon = eps[counter]\n",
    "            final.loc[i, epsilon] = diff\n",
    "            counter += 1\n",
    "    final.plot(x=\"gramtype\", y=[0.1, 1, 5, 10, 20], kind=\"line\", style='.-', figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_up(orig_name,filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Results description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots illustrate a similar trend and percentage error values (with the exception of epsilons 0.1 and 1 as those had mostly 0 values in their noisy datasets) to the other method used above: the higher the n, the higher the percentage error. Four grams, for the reason we are not sure about, are a low outlier in the trend, but five grams demonstrate around 75% error compared to 5 - 25% error for unigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the noises added by the Laplace mechanism is unbounded, noisy counts for each sequence varies significantly among multiple rounds. We therefore use Boxplot.py to generate 100 rounds of noisy counts for each epsilon value (0.1, 1, 5, 10, 20, 50) and examine the performance of the ngram algorithm based on the length of ngrams and magnitude of epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ''\n",
    "og = pd.read_csv('', sep = ':', dtype={'Seq': str})\n",
    "og['ngram'] = og['Seq'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Averaging Percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a particular epsilon and n, for each round, we 1) calculate the percentage error for each sequence, and 2) average the percentage errors of all sequences to derive avg_error. We then draw the boxplot of average percentage errors for 100 rounds. When we calculate the percentage error, the denominator is replaced by sanity_bound whenever the original count of that sequence is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Sanity Bound 0.008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avg_percentage_error_boxplot(epsilon, sanity_bound):\n",
    "    # epsilon options: 'eps_0.1', 'eps_1', 'eps_5', 'eps_10', 'eps_20', 'eps_50'\n",
    "    \n",
    "    ngrams = range(1, 5)\n",
    "    datasets = {}\n",
    "\n",
    "    # Read in all rounds for epsilon into datasets\n",
    "    for i in range(1,101):\n",
    "        file = epsilon + '-round_' + str(i) + '.dat'\n",
    "        name = 'round_ ' + str(i)\n",
    "        dataset = pd.read_csv(path + file, dtype={'Seq': str}, sep=':')\n",
    "        # File name exmaples: round_1 from path .../small_example-eps_0.1-round_1.dat\n",
    "        dataset['Seq'] = dataset['Seq'].astype(str) # Convert Seq column into strings\n",
    "        dataset['ngram'] = dataset['Seq'].apply(lambda x: len(str(x).split())) # Classiy ngram\n",
    "        datasets[name] = dataset\n",
    "    \n",
    "    # Initialize a figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Iterate over each n-gram\n",
    "    for n in ngrams:\n",
    "        error_distribution = []\n",
    "        \n",
    "        # Loop over each round, and find the average percentage error for that round\n",
    "        for dataset_name, dataset in datasets.items():\n",
    "            # Identify the n-grams in the original dataset\n",
    "            og_ngram = og[og['ngram'] == n]\n",
    "            \n",
    "            # Identify the n-grams\n",
    "            dataset_ngram = dataset[dataset['ngram'] == n]\n",
    "            \n",
    "            # Merge datasets - union sequences\n",
    "            difference = pd.merge(dataset_ngram, og_ngram, on='Seq', how='outer', suffixes=('_' + dataset_name, '_og'))\n",
    "\n",
    "            # Fill NaNs with 0\n",
    "            difference.fillna(0, inplace=True)\n",
    "\n",
    "            # Calculate the absolute difference\n",
    "            difference['abs_difference'] = np.abs(difference['Counts_og'] - difference['Counts_' + dataset_name])\n",
    "\n",
    "            # Calculate the percentage difference\n",
    "            difference['percentage_difference'] = difference.apply(lambda row: 100 * row['abs_difference'] / max(row['Counts_og'], sanity_bound), axis=1)\n",
    "\n",
    "            # Filter out seqs whose noisy counts are less than 1\n",
    "            difference = difference[difference['Counts_' + dataset_name] >= 1]\n",
    "\n",
    "            # Calculate the average percentage error for the current epsilon and n-gram\n",
    "            avg_error = np.mean(difference['percentage_difference'])\n",
    "            \n",
    "            if not np.isnan(avg_error):\n",
    "                # Append the average error to the list if avg_error is not nan\n",
    "                # avg_error is nan if percentage_difference col is empty after filter\n",
    "                error_distribution.append(avg_error)\n",
    "\n",
    "        # Plotting boxplots for each n-gram\n",
    "        plt.boxplot(error_distribution, positions=[n], labels=[n])\n",
    "\n",
    "    plt.xlabel(\"n-gram\")\n",
    "    plt.ylabel(\"Average Percentage Error\")\n",
    "    plt.title(f\"Average Percentage Error per n-gram for {epsilon}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = ['eps_0.1', 'eps_1', 'eps_5', 'eps_10', 'eps_20', 'eps_50']\n",
    "for epsilon in epsilons:\n",
    "    plot_avg_percentage_error_boxplot(epsilon, 0.008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We observe that as epsilon increases, the median average percentage error decreases, corresponding to greater privacy loss and decreasing magnitude of added noises.\n",
    "* As length of n-gram increases, the average percentage error generally increases. This is likely because a lot of fake sequences not present in the original dataset are created and their percentage errors are huge because we divide the noisy count by a small sanity bound of 0.008."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Sanity Bound 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = ['eps_0.1', 'eps_1', 'eps_5', 'eps_10', 'eps_20', 'eps_50']\n",
    "for epsilon in epsilons:\n",
    "    plot_avg_percentage_error_boxplot(epsilon, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As epsilon increases, the median percentage error first decreases and then plateaus after epsilon reaches 5.\n",
    "* As length of n-gram increases, the median percentage error fluctuates. It seems that the median percentage error is not entirely related to the length of n-gram, and this is contrary to our expectation that it will increase while length of n-gram increases. To address this, we apply variable sanity-bound in the following section, using average count per gram per n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Varying sanity bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ''\n",
    "og = pd.read_csv('small_example-original-5grams.dat', sep = ':', dtype={'Seq': str})\n",
    "# Classify n-grams\n",
    "og['ngram'] = og['Seq'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_counts(n):\n",
    "    ngram1 = og[og['ngram']==n]\n",
    "    avg_counts = np.sum(ngram1['Counts'])/len(ngram1['Counts'])\n",
    "    print(f'Average counts for {n}-gram: ', avg_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = range(1,5)\n",
    "for n in ngrams:\n",
    "    avg_counts(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avg_percentage_error_boxplot_v(epsilon, sanity_bounds):\n",
    "    # epsilon options: 'eps_0.1', 'eps_1', 'eps_5', 'eps_10', 'eps_20', 'eps_50'\n",
    "    \n",
    "    ngrams = range(1, 5)\n",
    "    datasets = {}\n",
    "\n",
    "    # Read in all rounds for epsilon into datasets\n",
    "    for i in range(1,101):\n",
    "        file = epsilon + '-round_' + str(i) + '.dat'\n",
    "        name = 'round_ ' + str(i)\n",
    "        dataset = pd.read_csv(path + file, dtype={'Seq': str}, sep=':')\n",
    "        # File name exmaples: round_1 from path .../small_example-eps_0.1-round_1.dat\n",
    "        dataset['Seq'] = dataset['Seq'].astype(str) # Convert Seq column into strings\n",
    "        dataset['ngram'] = dataset['Seq'].apply(lambda x: len(str(x).split())) # Classiy ngram\n",
    "        datasets[name] = dataset\n",
    "    \n",
    "    # Initialize a figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Iterate over each n-gram\n",
    "    for n in ngrams:\n",
    "        error_distribution = []\n",
    "        \n",
    "        # Loop over each round, and find the average percentage error for that round\n",
    "        for dataset_name, dataset in datasets.items():\n",
    "            # Identify the n-grams in the original dataset\n",
    "            og_ngram = og[og['ngram'] == n]\n",
    "            \n",
    "            # Identify the n-grams\n",
    "            dataset_ngram = dataset[dataset['ngram'] == n]\n",
    "            \n",
    "            # Merge datasets - union sequences\n",
    "            difference = pd.merge(dataset_ngram, og_ngram, on='Seq', how='outer', suffixes=('_' + dataset_name, '_og'))\n",
    "\n",
    "            # Fill NaNs with 0\n",
    "            difference.fillna(0, inplace=True)\n",
    "\n",
    "            # Calculate the absolute difference\n",
    "            difference['abs_difference'] = np.abs(difference['Counts_og'] - difference['Counts_' + dataset_name])\n",
    "\n",
    "            # Calculate the percentage difference\n",
    "            difference['percentage_difference'] = difference.apply(lambda row: 100 * row['abs_difference'] / max(row['Counts_og'], sanity_bounds[n-1]), axis=1)\n",
    "\n",
    "            # Filter out seqs whose noisy counts are less than 1\n",
    "            difference = difference[difference['Counts_' + dataset_name] >= 1]\n",
    "\n",
    "            # Calculate the average percentage error for the current epsilon and n-gram\n",
    "            avg_error = np.mean(difference['percentage_difference'])\n",
    "            \n",
    "            if not np.isnan(avg_error):\n",
    "                # Append the average error to the list if avg_error is not nan\n",
    "                # avg_error is nan if percentage_difference col is empty after filter\n",
    "                error_distribution.append(avg_error)\n",
    "\n",
    "        # Plotting boxplots for each n-gram\n",
    "        plt.boxplot(error_distribution, positions=[n], labels=[n])\n",
    "\n",
    "    plt.xlabel(\"n-gram\")\n",
    "    plt.ylabel(\"Average Percentage Error\")\n",
    "    plt.title(f\"Average Percentage Error per n-gram for {epsilon}, Adjusted Sanity Bound\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_bounds = [8,3,2,1.6]\n",
    "epsilons = ['eps_0.1', 'eps_1', 'eps_5', 'eps_10', 'eps_20', 'eps_50']\n",
    "for epsilon in epsilons:\n",
    "    plot_avg_percentage_error_boxplot_v(epsilon, sanity_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We observe that the average count per n-gram decreases as n increases\n",
    "* As we use the average count per n-gram type as the sanity bound, we observe the expected average percentag error trends:\n",
    "    * Median percentage error decreases as epsilon increases\n",
    "    * Median percentage error increases as length of n-gram increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Percentage Error of Sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way of calculating percentage error is to take the ratio of the sum of all noisy counts and the sum of original counts for a particular n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Filtered noisy counts less than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avg_percentage_error_boxplot_sums(epsilon, ngrams):\n",
    "    # epsilon options: 'eps_0.1', 'eps_1', 'eps_5', 'eps_10', 'eps_20', 'eps_50'\n",
    "\n",
    "    datasets = {}\n",
    "\n",
    "    # Read in all rounds for epsilon into datasets\n",
    "    for i in range(1,101):\n",
    "        file = epsilon + '-round_' + str(i) + '.dat'\n",
    "        name = 'round_ ' + str(i)\n",
    "        dataset = pd.read_csv(path + file, dtype={'Seq': str}, sep=':')\n",
    "        # File name exmaples: round_1 from path .../small_example-eps_0.1-round_1.dat\n",
    "        dataset['Seq'] = dataset['Seq'].astype(str) # Convert Seq column into strings\n",
    "        dataset['ngram'] = dataset['Seq'].apply(lambda x: len(str(x).split())) # Classiy ngram\n",
    "        datasets[name] = dataset\n",
    "    \n",
    "    # Initialize a figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Iterate over each n-gram\n",
    "    for n in ngrams:\n",
    "        error_distribution = []\n",
    "        \n",
    "        # Loop over each round, and find the average percentage error for that round\n",
    "        for dataset_name, dataset in datasets.items():\n",
    "            # Identify the n-grams in the original dataset\n",
    "            og_ngram = og[og['ngram'] == n]\n",
    "            \n",
    "            # Identify the n-grams\n",
    "            dataset_ngram = dataset[dataset['ngram'] == n]\n",
    "            \n",
    "            # Merge datasets - union sequences\n",
    "            difference = pd.merge(dataset_ngram, og_ngram, on='Seq', how='outer', suffixes=('_' + dataset_name, '_og'))\n",
    "\n",
    "            # Fill NaNs with 0\n",
    "            difference.fillna(0, inplace=True)\n",
    "\n",
    "            # Calculate the absolute difference\n",
    "            difference['abs_difference'] = np.abs(difference['Counts_og'] - difference['Counts_' + dataset_name])\n",
    "\n",
    "            # Filter out seqs whose noisy counts are less than 1\n",
    "            difference = difference[difference['Counts_' + dataset_name] >= 1]\n",
    "            \n",
    "            # Calculate og_counts total\n",
    "            og_counts_total = og_ngram['Counts'].sum()\n",
    "            \n",
    "            # Calculate the total absolute difference between sum of data set counts and sum of og counts\n",
    "            abs_difference = np.abs(og_counts_total - difference['Counts_' + dataset_name].sum())\n",
    "\n",
    "            # Calculate the percentage difference\n",
    "            percentage_difference = 100 * abs_difference / og_counts_total\n",
    "            \n",
    "            if not np.isnan(percentage_difference):\n",
    "                # Append the average error to the list if avg_error is not nan\n",
    "                # avg_error is nan if percentage_difference col is empty after filter\n",
    "                # print('abs_diff', abs_difference, 'og_counts_total', og_counts_total, 'perc_diff', percentage_difference)\n",
    "                error_distribution.append(percentage_difference)\n",
    "\n",
    "        # Plotting boxplots for each n-gram\n",
    "        plt.boxplot(error_distribution, positions=[n], labels=[n])\n",
    "\n",
    "    plt.xlabel(\"n-gram\")\n",
    "    plt.ylabel(\"Average Percentage Error\")\n",
    "    plt.title(f\"Average Percentage Error per n-gram for {epsilon}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = ['eps_0.1', 'eps_1', 'eps_5', 'eps_10', 'eps_20', 'eps_50']\n",
    "ngrams = range(1, 5)\n",
    "for epsilon in epsilons:\n",
    "    plot_avg_percentage_error_boxplot_sums(epsilon, ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 No filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_avg_percentage_error_boxplot_sums_2(epsilon, ngrams):\n",
    "    # epsilon options: 'eps_0.1', 'eps_1', 'eps_5', 'eps_10', 'eps_20', 'eps_50'\n",
    "\n",
    "    datasets = {}\n",
    "\n",
    "    # Read in all rounds for epsilon into datasets\n",
    "    for i in range(1,101):\n",
    "        file = epsilon + '-round_' + str(i) + '.dat'\n",
    "        name = 'round_ ' + str(i)\n",
    "        dataset = pd.read_csv(path + file, dtype={'Seq': str}, sep=':')\n",
    "        # File name exmaples: round_1 from path .../small_example-eps_0.1-round_1.dat\n",
    "        dataset['Seq'] = dataset['Seq'].astype(str) # Convert Seq column into strings\n",
    "        dataset['ngram'] = dataset['Seq'].apply(lambda x: len(str(x).split())) # Classiy ngram\n",
    "        datasets[name] = dataset\n",
    "    \n",
    "    # Initialize a figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Iterate over each n-gram\n",
    "    for n in ngrams:\n",
    "        error_distribution = []\n",
    "        \n",
    "        # Loop over each round, and find the average percentage error for that round\n",
    "        for dataset_name, dataset in datasets.items():\n",
    "            # Identify the n-grams in the original dataset\n",
    "            og_ngram = og[og['ngram'] == n]\n",
    "            \n",
    "            # Identify the n-grams\n",
    "            dataset_ngram = dataset[dataset['ngram'] == n]\n",
    "            \n",
    "            # Merge datasets - union sequences\n",
    "            difference = pd.merge(dataset_ngram, og_ngram, on='Seq', how='outer', suffixes=('_' + dataset_name, '_og'))\n",
    "\n",
    "            # Fill NaNs with 0\n",
    "            difference.fillna(0, inplace=True)\n",
    "\n",
    "            # Calculate the absolute difference\n",
    "            difference['abs_difference'] = np.abs(difference['Counts_og'] - difference['Counts_' + dataset_name])\n",
    "            \n",
    "            # Calculate og_counts total\n",
    "            og_counts_total = og_ngram['Counts'].sum()\n",
    "            \n",
    "            # Calculate the total absolute difference between sum of data set counts and sum of og counts\n",
    "            abs_difference = np.abs(og_counts_total - difference['Counts_' + dataset_name].sum())\n",
    "\n",
    "            # Calculate the percentage difference\n",
    "            percentage_difference = 100 * abs_difference / og_counts_total\n",
    "            \n",
    "            if not np.isnan(percentage_difference):\n",
    "                # Append the average error to the list if avg_error is not nan\n",
    "                # avg_error is nan if percentage_difference col is empty after filter\n",
    "                # print('abs_diff', abs_difference, 'og_counts_total', og_counts_total, 'perc_diff', percentage_difference)\n",
    "                error_distribution.append(percentage_difference)\n",
    "\n",
    "        # Plotting boxplots for each n-gram\n",
    "        plt.boxplot(error_distribution, positions=[n], labels=[n])\n",
    "\n",
    "    plt.xlabel(\"n-gram\")\n",
    "    plt.ylabel(\"Average Percentage Error\")\n",
    "    plt.title(f\"Average Percentage Error per n-gram for {epsilon}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = ['eps_0.1', 'eps_1', 'eps_5', 'eps_10', 'eps_20', 'eps_50']\n",
    "ngrams = range(1, 5)\n",
    "for epsilon in epsilons:\n",
    "    plot_avg_percentage_error_boxplot_sums_2(epsilon, ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As epsilon increases, median average percentage error first decreases and then plateaus as epsilon turns 5\n",
    "* There is no clear correlation between the length of ngrams and the average percentage errors."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
